{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering\n",
    "## Credit Scoring Model Project\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Handle missing values systematically\n",
    "- Create domain-based features using business knowledge\n",
    "- Encode categorical variables appropriately\n",
    "- Scale and normalize numerical features\n",
    "- Perform feature selection\n",
    "- Prepare data for modeling\n",
    "\n",
    "**What is Feature Engineering?**\n",
    "Feature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. It's often the difference between a mediocre model and a great one!\n",
    "\n",
    "**Quote:** \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" - Andrew Ng\n",
    "\n",
    "Let's build powerful features!\n",
    "\n",
    "## \ud83d\udce6 Import Libraries and Utilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our custom utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data_preprocessing import (\n",
    "    load_data,\n",
    "    analyze_missing_values,\n",
    "    handle_missing_values,\n",
    "    validate_data_quality\n",
    ")\n",
    "from src.domain_features import create_domain_features\n",
    "from src.feature_engineering import encode_categorical_features, clean_column_names, scale_features\n",
    "from src.feature_selection import select_features\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"[OK] All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udcc2 Load Data\n",
    "\n",
    "We\\'ll load the data we explored in the EDA notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using our utility\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['TARGET'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udd0d Handle Missing Values\n",
    "\n",
    "**Strategy:**\n",
    "1. Identify features with excessive missing values (>70%) \u2192 Consider dropping\n",
    "2. Create missing indicators for important features\n",
    "3. Impute remaining missing values appropriately\n",
    "\n",
    "**Educational Note:**\n",
    "Different imputation strategies work for different scenarios:\n",
    "- **Median:** For numerical features with outliers (robust)\n",
    "- **Mean:** For numerical features with normal distribution\n",
    "- **Mode:** For categorical features\n",
    "- **Constant:** When missingness has meaning (e.g., 0 for no car)\n",
    "- **Missing Indicator:** Preserve information about missingness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "missing_summary = analyze_missing_values(train_df, threshold=0)\n",
    "\n",
    "# Separate features by missing percentage\n",
    "high_missing = missing_summary[missing_summary['missing_percent'] > 70]['column'].tolist()\n",
    "medium_missing = missing_summary[(missing_summary['missing_percent'] > 20) &\n",
    "                                 (missing_summary['missing_percent'] <= 70)]['column'].tolist()\n",
    "low_missing = missing_summary[(missing_summary['missing_percent'] > 0) &\n",
    "                              (missing_summary['missing_percent'] <= 20)]['column'].tolist()\n",
    "\n",
    "print(f\"\\n[DECISION SUMMARY]\")\n",
    "print(f\"High missing (>70%): {len(high_missing)} features - CONSIDER DROPPING\")\n",
    "print(f\"Medium missing (20-70%): {len(medium_missing)} features - CREATE INDICATORS + IMPUTE\")\n",
    "print(f\"Low missing (<20%): {len(low_missing)} features - IMPUTE ONLY\")\n",
    "\n",
    "# Let\\'s drop very sparse features\n",
    "print(f\"\\nDropping {len(high_missing)} features with >70% missing...\")\n",
    "train_df = train_df.drop(columns=high_missing)\n",
    "test_df = test_df.drop(columns=[col for col in high_missing if col in test_df.columns])\n",
    "\n",
    "print(f\"New training shape: {train_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83c\udfaf Create Domain-Based Features\n",
    "\n",
    "**Domain Knowledge in Credit Scoring:**\n",
    "Key financial ratios and indicators matter:\n",
    "- **Debt-to-Income Ratio:** How much debt relative to income?\n",
    "- **Credit Utilization:** How much of available credit is used?\n",
    "- **Payment Burden:** Can they afford the payments?\n",
    "- **Age/Employment Stability:** Risk indicators\n",
    "\n",
    "Let\\'s create features that capture these concepts!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our new modular function\n",
    "print(\"=\"*80)\n",
    "train_df = create_domain_features(train_df)\n",
    "test_df = create_domain_features(test_df)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nNew shape after feature creation:\")\n",
    "print(f\"Training: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83c\udff7\ufe0f Encode Categorical Variables\n",
    "\n",
    "**Encoding Strategies:**\n",
    "\n",
    "1. **Label Encoding:** For ordinal categories (order matters)\n",
    "   - Example: Education level (Low \u2192 Medium \u2192 High)\n",
    "\n",
    "2. **One-Hot Encoding:** For nominal categories (no order)\n",
    "   - Example: Contract type (Cash, Revolving)\n",
    "   - Creates binary columns for each category\n",
    "\n",
    "3. **Target Encoding:** For high-cardinality features\n",
    "   - Encode by mean target value per category\n",
    "   - Use with caution (risk of overfitting!)\n",
    "\n",
    "**Rule of Thumb:**\n",
    "- Low cardinality (<10 categories) \u2192 One-Hot Encoding\n",
    "- High cardinality (>10 categories) \u2192 Target Encoding or drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our new modular function\n",
    "train_df, test_df = encode_categorical_features(train_df, test_df, cardinality_limit=10)\n",
    "\n",
    "# Clean column names for LightGBM compatibility\n",
    "print(\"\\nCleaning column names...\")\n",
    "train_df = clean_column_names(train_df)\n",
    "test_df = clean_column_names(test_df)\n",
    "\n",
    "print(f\"\\nFinal shape after handling all categoricals:\")\n",
    "print(f\"Training: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udd27 Impute Remaining Missing Values\n",
    "\n",
    "Now we\\'ll impute the remaining missing values using appropriate strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing values\n",
    "missing_cols = train_df.columns[train_df.isnull().any()].tolist()\n",
    "if 'TARGET' in missing_cols:\n",
    "    missing_cols.remove('TARGET')\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "\n",
    "# Separate numerical and categorical\n",
    "numerical_missing = [col for col in missing_cols\n",
    "                     if train_df[col].dtype in ['int64', 'float64']]\n",
    "categorical_missing = [col for col in missing_cols\n",
    "                       if train_df[col].dtype == 'object']\n",
    "\n",
    "print(f\"  Numerical: {len(numerical_missing)}\")\n",
    "print(f\"  Categorical: {len(categorical_missing)}\")\n",
    "\n",
    "# Impute numerical with median\n",
    "if numerical_missing:\n",
    "    print(f\"\\nImputing {len(numerical_missing)} numerical features with median...\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    train_df[numerical_missing] = imputer.fit_transform(train_df[numerical_missing])\n",
    "    test_df[numerical_missing] = imputer.transform(test_df[numerical_missing])\n",
    "    print(\"  [OK] Numerical imputation complete\")\n",
    "\n",
    "# Impute categorical with most frequent\n",
    "if categorical_missing:\n",
    "    print(f\"\\nImputing {len(categorical_missing)} categorical features with mode...\")\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    train_df[categorical_missing] = imputer.fit_transform(train_df[categorical_missing])\n",
    "    test_df[categorical_missing] = imputer.transform(test_df[categorical_missing])\n",
    "    print(\"  [OK] Categorical imputation complete\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\n[VERIFICATION]\")\n",
    "print(f\"Training missing: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Test missing: {test_df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83c\udfaf Feature Selection\n",
    "\n",
    "**Why Feature Selection?**\n",
    "1. Reduce overfitting (simpler models generalize better)\n",
    "2. Improve model performance (remove noise)\n",
    "3. Reduce training time\n",
    "4. Improve interpretability\n",
    "\n",
    "**Strategies:**\n",
    "1. Remove low-variance features (constant or near-constant)\n",
    "2. Remove highly correlated features (redundant information)\n",
    "3. Use feature importance from baseline models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "y_train = train_df['TARGET']\n",
    "X_test = test_df.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "# Use our new modular function\n",
    "X_train, X_test = select_features(X_train, X_test)\n",
    "\n",
    "print(f\"[SUCCESS] Removed {train_df.shape[1] - X_train.shape[1] - 2} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \u2696\ufe0f Scale Features\n",
    "\n",
    "**Why Scaling?**\n",
    "- Features have different ranges (income vs children count)\n",
    "- Many ML algorithms are sensitive to feature scales\n",
    "- Required for: Logistic Regression, SVM, Neural Networks\n",
    "- Optional for: Tree-based models (Random Forest, XGBoost)\n",
    "\n",
    "**Scaling Methods:**\n",
    "1. **StandardScaler:** Mean=0, Std=1 (use when data is normally distributed)\n",
    "2. **MinMaxScaler:** Scale to [0, 1] (use when data has outliers)\n",
    "3. **RobustScaler:** Uses median and IQR (robust to outliers)\n",
    "\n",
    "We\\'ll use StandardScaler as it works well for most cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our new modular function\n",
    "X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"\\nScaled feature statistics:\")\n",
    "print(X_train_scaled.describe().loc[['mean', 'std']].round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udd00 Create Train-Validation Split\n",
    "\n",
    "**Important:** Use **STRATIFIED** split to preserve class distribution!\n",
    "\n",
    "We\\'ll create:\n",
    "- Training set (70%): For training models\n",
    "- Validation set (30%): For hyperparameter tuning and model selection\n",
    "- Test set: Already separate (for final evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    test_size=0.3,\n",
    "    stratify=y_train,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Train-Validation Split:\")\n",
    "print(f\"  Training: {X_train_split.shape}\")\n",
    "print(f\"  Validation: {X_val_split.shape}\")\n",
    "print(f\"  Test: {X_test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "print(f\"  Original: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  Training: {y_train_split.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  Validation: {y_val_split.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "print(\"\\n[OK] Class distribution preserved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udcbe Save Processed Data\n",
    "\n",
    "Save the processed data so we can use it in the next notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "X_train_split.to_csv(processed_dir / 'X_train.csv', index=False)\n",
    "X_val_split.to_csv(processed_dir / 'X_val.csv', index=False)\n",
    "X_test_scaled.to_csv(processed_dir / 'X_test.csv', index=False)\n",
    "\n",
    "y_train_split.to_csv(processed_dir / 'y_train.csv', index=False, header=True)\n",
    "y_val_split.to_csv(processed_dir / 'y_val.csv', index=False, header=True)\n",
    "\n",
    "# Save feature names\n",
    "pd.DataFrame({'feature': X_train_split.columns}).to_csv(\n",
    "    processed_dir / 'feature_names.csv', index=False\n",
    ")\n",
    "\n",
    "# Save IDs\n",
    "train_df[['SK_ID_CURR']].iloc[X_train_split.index].to_csv(\n",
    "    processed_dir / 'train_ids.csv', index=False\n",
    ")\n",
    "train_df[['SK_ID_CURR']].iloc[X_val_split.index].to_csv(\n",
    "    processed_dir / 'val_ids.csv', index=False\n",
    ")\n",
    "test_df[['SK_ID_CURR']].to_csv(processed_dir / 'test_ids.csv', index=False)\n",
    "\n",
    "print(\"[OK] All datasets saved!\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - X_train.csv: {X_train_split.shape}\")\n",
    "print(f\"  - X_val.csv: {X_val_split.shape}\")\n",
    "print(f\"  - X_test.csv: {X_test_scaled.shape}\")\n",
    "print(f\"  - y_train.csv, y_val.csv\")\n",
    "print(f\"  - feature_names.csv: {len(X_train_split.columns)} features\")\n",
    "print(f\"  - IDs for each split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udcdd Feature Engineering Summary\n",
    "\n",
    "### \u2705 What We Accomplished\n",
    "\n",
    "1. **Handled Missing Values**\n",
    "   - Dropped features with >70% missing\n",
    "   - Created missing indicators\n",
    "   - Imputed remaining values\n",
    "\n",
    "2. **Created Domain Features** (10+ new features)\n",
    "   - Age and employment features\n",
    "   - Income per person\n",
    "   - Debt-to-income ratio (KEY!)\n",
    "   - Credit utilization\n",
    "   - Payment burden ratio\n",
    "   - Family features\n",
    "   - Document counts\n",
    "   - External source aggregations\n",
    "\n",
    "3. **Encoded Categorical Variables**\n",
    "   - One-hot encoded low-cardinality features\n",
    "   - Handled high-cardinality features\n",
    "\n",
    "4. **Feature Selection**\n",
    "   - Removed low-variance features\n",
    "   - Removed highly correlated features\n",
    "   - Reduced feature count significantly\n",
    "\n",
    "5. **Scaled Features**\n",
    "   - StandardScaler (mean=0, std=1)\n",
    "   - Ready for modeling!\n",
    "\n",
    "6. **Created Train-Val Split**\n",
    "   - Stratified sampling\n",
    "   - 70% training, 30% validation\n",
    "\n",
    "### \ud83c\udfaf Key Features Created\n",
    "\n",
    "The most important features for credit scoring:\n",
    "1. **DEBT_TO_INCOME_RATIO** - How much debt vs income\n",
    "2. **ANNUITY_TO_INCOME_RATIO** - Can afford payments?\n",
    "3. **EMPLOYMENT_YEARS** - Stability indicator\n",
    "4. **AGE_YEARS** - Risk correlates with age\n",
    "5. **INCOME_PER_PERSON** - Family financial situation\n",
    "6. **CREDIT_UTILIZATION** - Credit usage patterns\n",
    "\n",
    "### \ud83d\udcca Final Dataset\n",
    "\n",
    "- **Features:** ~100-150 (after selection and engineering)\n",
    "- **Training samples:** ~215,000\n",
    "- **Validation samples:** ~92,000\n",
    "- **Test samples:** ~48,000\n",
    "- **Class balance:** ~8% positive class (defaults)\n",
    "\n",
    "### \ud83d\ude80 Next Steps\n",
    "\n",
    "In the next notebook ([03_baseline_models.ipynb](03_baseline_models.ipynb)), we will:\n",
    "\n",
    "1. **Train Multiple Baseline Models**\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "\n",
    "2. **Set Up MLflow Tracking**\n",
    "   - Log all experiments\n",
    "   - Compare models\n",
    "   - Save artifacts\n",
    "\n",
    "3. **Evaluate Using Appropriate Metrics**\n",
    "   - ROC-AUC\n",
    "   - Precision-Recall AUC\n",
    "   - F1-Score\n",
    "   - Confusion Matrix\n",
    "\n",
    "4. **Select Best Baseline Model**\n",
    "   - Compare performance\n",
    "   - Consider interpretability\n",
    "   - Choose for optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work on feature engineering! \ud83c\udf89**\n",
    "\n",
    "Your data is now ready for modeling. Remember: good features are often more important than complex models!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}